# 머신러닝의 기본 프로세스

1. 학습하고자 하는 가설을 수학적 표현식으로 나타내기

2. 가설의 성능을 측정할 수 있는 손실함수를 정의

3. 손실 함수를 최소화 할 수 있는 알고리즘 설계

위의 과정들을 통해서 머신러닝 학습을 진행하게 됩니다.\
아래에서 순서대로 프로세스에 해당하는 내용을 알아보도록 하겠습니다.

<br><br>

## 1. 모델

머신러닝 모델 중 가장 간단한 모델은 `선형 회기 Linear Regression` 모델이 있습니다.

`회기` 란 어떠한 `실수값(1.2, 2.6...)` 을 예측하는 문제를 의미합니다.\
예측하는 값이 실수값이 아닌 `이산값(0, 1, 2, 3...)` 이라면 `분류` 라고 합니다.

선형 회기 모델은 선형 함수를 이용하여 회귀를 수행하게 됩니다.

**`y = Wx + b`**

위의 선형함수에서 x는 입력으로 주어지는 데이터, y는 입력으로 주어지는 정답(label) 입니다.\
`W` 와 `b` 는 파라미터이며 여러 입력 데이터들로 학습을 진행하여 적절한 W와 b의 값을 찾아나갑니다.

<br>

## 2. 손실 함수

학습 중 찾아낸 파라미터 값이 우리가 찾아야하는 최적의 값인지 확인하기 위하여 손실 함수를 이용합니다.\
대표적인 손실 함수로 `평균제곱오차 (Mean of Squared Error, MSE)` 가 있습니다.

`MSE` 는 모델의 예측값과 실제 정답(레이블)과의 차이를 제곱하여 모두 더한 후 평균을 취하는 함수입니다.\
예측값이 실제값과 비슷할수록 손실 함수의 값은 작은 값을 갖게 됩니다.

학습을 진행하면서 손실 값이 작은 파라미터를 계속해서 찾아나가게 됩니다.\
손실 함수를 다른 말로 `비용 함수` 라고도 부릅니다.

<br>

## 3. 최적화 기법

손실 함수의 값을 최소화 하는 방향으로 파라미터를 업데이트 해주어야 합니다.\
파라미터를 적절한 값으로 업데이트 해주는 알고리즘을 `최적화 기법` 이라고 합니다.

여러 최적화 기법 중 대표적인 기법은 `경사하강법 Gradient Descent` 입니다.\
이 외에도 `Newton's Method`, `Genetic Algorithm`, `Simulated Annealing` 등이 있습니다.

경사하강법은 현재 파라미터에서 손실 함수의 미분값에 `러닝레이트(learning rate)` 를 곱한만큼 빼서 다음 파라미터값을 찾습니다.\
러닝레이트 값은 임의로 작성하여 절절한 수준을 선택하여야 합니다.\
러닝레이트가 너무 크면 학습 시간이 단축되지만, 세세한 최적의 파라미터를 찾기 어렵습니다.\
반대로 러닝레이트가 너무 작으면 세세한 최적의 파라미터 값을 찾기 용이하지만, 학습 시간이 길어집니다.

경사하강법은 아래의 그림처럼 `Global Minimum` 값을 찾아야 하지만, `Local Minimum` 구역에 빠져 최소값을 찾지 못하는 문제가 발생 할 수 있습니다.

![경사하강법 단점](http://wiki.hash.kr/images/e/ea/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95.png)

출처 : http://wiki.hash.kr/index.php/%ED%99%95%EB%A5%A0%EC%A0%81_%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95

<br>

### Batch, Mini-Batch, Stochastic Gradient Descent

- **Batch Gradient Descent**

    경사하강법에서 다음 스텝으로 값을 업데이트 할 때, 전체 학습 데이터를 하나의 Batch로 만들어 사용하면 `Batch Gradient Descent` 라고 부릅니다.\
    모든 데이터를 하나의 Batch로 묶어서 사용하면 모든 데이터의 미분값을 계산하고 한단계로 업데이트가 이루어지기 때문에 많은 시간이 소요됩니다.

- **Stochastic Gradient Descent**

    위의 Batch Gradient Descent 와 다르게 다음 스텝으로 업데이트 할때 1개의 학습 데이터만 사용하여 업데이트를 진행하는 경우에 `Stochastic Gradient Descent` 라고 부릅니다.\
    한번 업데이트 할때 1개의 데이터만 이용하여 업데이트를 진행하게 되면 전체 데이터의 특성을 고려하지 않고, 단 하나의 데이터 특성만 고려하므로 업데이트가 옳지 않은 방향으로 진행될 수 있습니다.

- **Mini-Batch Gradient Descent**

    위의 `Batch Gradient Descent` 와 `Stochastic Gradient Descent` 의 중간 정도인 `Mini-Batch Gradient Descent` 가 있습니다.\
    이 방법은 전체 학습 데이터가 1000개라면, 이 데이터들을 100개씩 묶어서 파라미터 값을 업데이트하는 방식입니다.

<br><br>

## 기초 이론들

### 데이터 분류

데이터는 모델 학습에 필요한 `트레이닝 데이터` 와 모델이 잘 학습 되었는지 확인할 때 사용하는 `테스트 데이터` 와 트레이닝 과정에서 학습하는 모델이 오버피팅에 빠지지 않았는지 확인하는데 사용하는 `검증용 데이터` 가 있습니다.

- **오버피팅**\
    모델이 학습 데이터 특징에 과하게 최적화가 되어 학습 데이터에서는 정상적으로 작동하지만, `새로운 데이터에 대해서는 잘 동작하지 못하는 현상을 의미합니다.

- **언더피팅**\
    오버피팅과는 반대로 모델의 표현력이 부족하여 학습 데이터조차 잘 구분하지 못하는 현상을 의미합니다.

딥러닝의 경우 모델의 표현력이 강하여 오버피팅에 빠지기 쉽습니다.\
오버피팅 문제를 완화하기 위하여 `ReLU` 나 `Dropout` 과 같은 다양한 `오버피팅 방지를 위한 기법 Regulatization` 들을 사용합니다.

<br>

### 소프트맥스 회귀 Softmax Regression

소프트맥스 회귀는 n개의 정답(레이블)을 분류하기 위한 가장 기본적인 모델입니다.\
모델의 출력에 소프트맥스 함수를 적용하여 출력값이 `각 정답(레이블)에 대해 확신하는 정도(확률)`을 출력하도록 하는 기법입니다.\
`소프트맥스 함수`는 `Normalization 함수`로 출력값들의 합을 1로 만들게 됩니다.

0~9까지 숫자를 구분하는 모델이라면 각 출력의 레이블에 대해 확률들을 보여줍니다.\
0일 확률, 1일 확률, 2일 확률...

<br>

### 크로스 엔트로피 손실 함수 Cross-Entropy

`분류 문제`에는 `크로스 엔트로피 Cross-Entropy` 손실 함수를 많이 사용합니다.\
분류 문제에서는 이전의 MSE 손실 함수보다 학습이 더 잘되는것으로 알려져 있습니다.

<br>

### One-hot Encoding

`One-hot Encoding` 은 범주형 값을 이진화된 값으로 바꿔서 표현하는것을 의미합니다.\
범주형 데이터 사과, 귤, 배와 같은 데이터를 단순히 숫자 인덱스 1, 2, 3 으로 나타내는 경우 학습에 오류가 생길 수 있습니다.\
이로 인해 머신 러닝 알고리즘 구현시 One-hot Encoding 형태로 표현하는것이 일반적입니다.

![One-hot Encoding](https://mblogthumb-phinf.pstatic.net/MjAyMDAzMDFfMjgx/MDAxNTgzMDIyNzg0MTc2.yzaPKdZzM5RdLJor5Ps0KfU5w3TWHU2cnXFDkTczOKIg.z4NJn4MGiwF49FmX6Mg7qgouuNhhlL1CivvrG0Bza1og.PNG.handuelly/image.png?type=w800)

출처 : https://brunch.co.kr/@sonnykim/6

<br><br>

---

## 📚 참고자료

- 도서 <텐서플로로 배우는 딥러닝>